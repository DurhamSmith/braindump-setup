:PROPERTIES:
:ID:       5a6f15fa-e5d4-474e-8ead-56b22d890512
:END:
#+title: Machine Learning
#+bibliography: biblio.bib
#+cite_export: csl

* When do we need machine learning?

Two aspects of a given problem may call for the use of programs that
learn and improve on the basis of their "experience":

1. *The problem's complexity*: some tasks that require elaborate
   introspection that cannot be well-defined in programs, such as
   driving, are ill-suited for coding by hand. Tasks that are beyond
   human capabilities such as the analysis of large datasets also fall
   in this category.

2. *Adaptivity*: Programmed tools are rigid, while machine learning
   tools allows for adaptation to the environment they interact with.

* Training vs Testing
- [[id:9e40e695-6a07-42e4-863a-8ce253e33757][Generalisation Error]]

** Growth Function
The /growth function/ is the quantity that will formalize the
effective number of hypotheses.

Each $h \in H$ generates a dichotomy which is $h$ is $-1$ or $h$ i-
$+1$. We then formally define dichotomies as follows:

\begin{align}
H(x_1, \dots, x_n) = \left\{ h(x_1), h(x_2), \dots, h(x_n) | h \in H \right\}
\end{align}

* Refile
** Data Compression
In /lossy compression/, we seek to trade off code length with
reconstruction error.

In /vector quantization/, we seek a small set of vectors ${z_i}$ to
describe a large dataset of vectors ${x_i}$, such that we can
represent each $x-i$ with its closest approximation in ${z_i}$ with
small error. (Clustering problem)

In /transform coding/, we transform the data, usually using a linear
tranformation. The data in the transformed domain is quantized,
usually discarding the small coefficients, corresponding to removing
some of the dimensions.
** Generative Learning Algorithms
Discriminative algorithms model $p(y | x)$ directly from the training
set.

Generative algorithms model $p(y | x)$ and $p(y)$. Then $argmax_y
p(y|x) = argmax_y \frac{p(x|y)p(y)}{p(x)} = argmax_y p(x|y)p(y)$.


*** Multivariate Normal Distribution
:PROPERTIES:
:ID:       9aa01f4a-0432-42d6-855a-cf17721449a1
:END:
A multivariate normal distribution is parameterized by a mean vector
$\mu \in R^n$ and a covariance matrix $\Sigma \in R^{n \times n}$, where $\Sigma \ge
0$ is symmetric and positive semi-definite.

*** TODO Gaussian Discriminant Analysis
In Gaussian Discriminant Analysis, p(x | y) is distributed to a
[[id:9aa01f4a-0432-42d6-855a-cf17721449a1][Multivariate Normal Distribution]].

\begin{align}
  y &\sim Bernoulli(\phi) \\
  x|y = 0 &\sim N(\mu_0, \Sigma) \\
  x|y = 1 &\sim N(\mu_1, \Sigma)
\end{align}

We can write out the distributions:

\begin{align}
  p(y) &= \phi^y (1 - \phi)^{1-y} \\
  p(x | y = 0) &= \frac{1}{(2\pi)^{n/2}|\Sigma|^{n/2}} exp \left( - \frac{1}{2} (x - \mu_0)^T \Sigma^{-1}(x - \mu_0) \right) \\
  p(x | y = 1) &= \frac{1}{(2\pi)^{n/2}|\Sigma|^{n/2}} exp \left( - \frac{1}{2} (x - \mu_1)^T \Sigma^{-1}(x - \mu_1) \right)
\end{align}

Then, the log-likelihood of the data is:

\begin{align}
  l(\phi, \mu_0, \mu_1, \Sigma) &= \log \prod_{i=1}^m p(x^{(i)}, y^{(i)}; \mu_0, \mu_1, \Sigma) \\
  &= \log \prod_{i=1}^m p(x^{(i) }| y^{(i)}; \mu_0, \mu_1, \Sigma)p(y^{(i)}; \phi)
\end{align}

We maximize $l$ with respect to the parameters.

* The Natural Language Decathlon: Multitask Learning as Question Answering: Richard Socher
[[https://einstein.ai/static/images/pages/research/decaNLP/decaNLP.pdf][paper]]

- Joint work with Bryan McCann, Nitish Keskar and Caiming Xiong

** Limits of Single-task Learning

- We can hill climb to local optima if $|dataset| > 100 \times C$
- For more general model, we need continuous learning in a single model

For pre-training in NLP, we're still stuck at the word vector level.
This compared to vision, where most of the model can be pre-trained,
only retraining the final few layers.

** Why has weight & model sharing not happened so much in NLP?
1. NLP requires many types of reasoning: logical, linguistic etc.
2. Requires short and long-term memory
3. NLP has been divided into intermediate and separate tasks to make
   progress (Benchmark chasing in each community)
5. Can a single unsupervised task solve it all? No, language clearly
   requires supervision in nature.

** Motivation for Single Multitask model

1. Step towards AGI
2. Important building block for:
   1. Sharing weights
   2. Transfer learning
   3. Zero-shot learning
   4. Domain adaptation
3. Easier deployment in production
4. Lowering the bar for anybody to solve their NLP task

End2end model vs parsing as intermediate step (e.g. running POS tagger
first).

** The 3 equivalent supertasks of NLP

Any NLP task can be mapped to these 3 super tasks:

1. Language Modeling
2. Question Answering
3. Dialogue

** Multitask learning as QA
- Question Answering
- Machine Translation
- Summarization
- NLI
- Sentiment Classification
- Semantic Role Labeling
- Relation Extraction

Meta supervised learning: {x, y} to {x, t, y}

** Designing a model for decaNLP
- No task-specific modules or parameters because task ID assumed to be unavailable

#+downloaded: /tmp/screenshot.png @ 2018-10-02 14:52:23
[[file:images/machine_learning/screenshot_2018-10-02_14-52-23.png]]

1. Start with a context
2. Ask a question
3. Generate answer one at a time by
   1. Pointing to context
   2. Pointing to question
   3. Choosing a word 

** Learnings
- Transformer Layers yield benefits in single-task and multitask
  setting
- QA and SRL have strong connections
- Pointing to the question is essential, despite the task being just
  classification for some subtasks
- Mulitasking helps a lot with zero-shot tasks

(Latest version of the paper coming out soon -- ICLR 2018)

** Training Strategies
- Fully Joint
- Curriculum learning doesn't work
- Anti-curriculum training works instead
  - Start with a really hard task


* Structuring Data Science Projects
Cookiecutter Data Science provides a decent project structure, and
uses the ubiquitous build tool ~Make~ to build data projects. [cite:@home_cookiec_data_scien]

#+begin_src text
├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.py           <- Make this project pip installable with `pip install -e`
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
└── tox.ini            <- tox file with settings for running tox; see tox.testrun.org
#+end_src

Stripe's approach [cite:@dan_reprod] still primarily uses Jupyter notebooks, but
has 2 main points. First, they strip the results from the Jupyter notebooks
before committing. Second, they ensure that the notebooks can be reproduced on
the work laptops and on their cloud infrastructure.

* Footnotes
[fn:1] See [[https://drivendata.github.io/cookiecutter-data-science/][Home - Cookiecutter Data Science]].
