:PROPERTIES:
:ID:       32f08e1c-16ac-4277-b4ad-1618b9f4a803
:END:
#+title: ID3 algorithm

ID3 learns decision trees by constructing them top down. Each instance
attribute is evaluated using a statistical test to determine how well
it alone classifies the examples. The best attribute is selected and
used as the test at the root node of the tree.

* Which is the best attribute?
:PROPERTIES:
:ID:       a7561a68-885f-4912-b1e3-33734f7dc237
:END:

 [[id:df3b1533-2dfc-4ea4-81f5-de61f77110d7][Information gain]] measures how well a given attribute separates the training
examples according to their target classification.

For example:

\begin{align}
  Values(Wind) &= Weak, Strong \\
  S &= [9+, 5-] \\
  S_{Weak} &\leftarrow [6+, 2-] \\
  S_{Strong} &\leftarrow [3+, 3-] \\
  Gain(S, Wind) &= Entropy(S) - \frac{8}{14}Entropy(S_{Weak}) -
                  \frac{6}{14}Entropy(S_{Strong}) \\
               &=0.048
\end{align}
* Hypothesis Space Search
ID3 can be characterised as searching a space of hypotheses for one
that fits the training examples. The hypothesis space searched is the
set of possible decision trees. ID3 performs a simple-to-complex,
hill-climbing search. The evaluation measure that guides the search is
the information gain measure.

Because ID3's hypothesis space of all decision trees is a complete
space of finite discrete-valued functions, it avoids the risk that the
hypothesis space might not contain the target function.

ID3 maintains only a single hypothesis as it searches through the
space of decision trees. ID3 loses the capabilities that follow from
explicitly representing all consistent hypothesis.

ID3 in its pure form performs no backtracking in its search, and can
result in locally but not globally optimal target functions.

ID3 uses all training examples at each step to make statistically
based decisions, unlike other algorithms that make decisions incrementally.

The [[id:103b141a-045b-43f1-bb78-09811bdccaf9][inductive bias]] of decision tree learning is that shorter trees are
preferred over larger trees (Occam's razor). Trees that place high
information gain attributes close to the root are preferred over those
that do not. ID3 can be viewed as a greedy heuristic search for the
shortest tree without conducting the entire breadth-first search
through the hypothesis space.

Notice that ID3 searches a complete hypothesis space incompletely, and
candidate-elimination searches an incomplete hypothesis space
completely. The inductive bias of ID3 follows from its search strategy
(/preference bias/), while that of candidate elimination follows from
the definition of its search space. (/restriction bias/).

* Why Prefer Shorter Hypotheses?

Fewer shorter hypothesis than larger ones, means it's less likely to over-generalise
